{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for MP\n",
    "\n",
    "**(d)** (N points) Let's add some extra features. Use the features that you believe will help detecting code clone. (For example, the count of Java keywords: specifically, for a Java function, how many 'if', how many 'try'? ... and so on.) Please feel free to design your own features.\n",
    "\n",
    "Here are several steps you might want to take to add the new features (you are welcomed to use different approaches):\n",
    "* Write a function that when given a code snippet, returns the features.\n",
    "* Write a function to create a dataframe for the features you designed.\n",
    "* Write a function that uses ``ColumnTransformer`` to combine unigram and the designed features.\n",
    "* Re-train your logistic regression model over the combined features.\n",
    "\n",
    " What is the 10-fold cross validation accuracy? (Make sure to use the same folds as before, so the results are comparable!)\n",
    " \n",
    "**(e)** (N points) Finally, some analysis: essentially, we have trained two models: LR with unigrams and bigrams, and LR with unigrams and features you designed. This setting allows us to compare the power of the new features (bigrams/features you designed). Which one is the better feature for the code clone detection task? Explain why this may be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR with unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "dataset = load_files(\"code-clone/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_val_score(df, vectorizer):\n",
    "    model = LogisticRegression(random_state=3, solver='liblinear', penalty=\"l1\")\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(df)\n",
    "    y_train = dataset.target\n",
    "    \n",
    "    lr_cross_val_score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (lr_cross_val_score.mean(), lr_cross_val_score.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7680 (+/- 0.0784)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "evaluate_cross_val_score(dataset.data, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR with extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (only unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7680 (+/- 0.0709)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "evaluate_cross_val_score(dataset.data, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Counting the number of classes in each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given code from sklearn.datasets.load_files(), extract list of unique class names.\n",
    "# Current strategy is look for 'new', and the word after it is probably a class name.\n",
    "def extract_classes(code):\n",
    "    retVal = set()\n",
    "    for file in code:\n",
    "        tokens = word_tokenize(file.decode())\n",
    "        for index, token in enumerate(tokens):\n",
    "            if token == 'new':\n",
    "                retVal.add(tokens[index+1])\n",
    "    return retVal\n",
    "\n",
    "class SimilarityScorer(BaseEstimator):\n",
    "    def fit(self, data, unused):\n",
    "        self.unique_classes = extract_classes(data)\n",
    "        self.unique_classes.update({\"byte\", \"short\", \"int\", \"long\", \"float\", \"double\", \"boolean\", \"String\"})\n",
    "        return self\n",
    "        \n",
    "    def transform(self, doc):\n",
    "        retVal = []\n",
    "        \n",
    "        \n",
    "        for file in doc:\n",
    "            tokens = word_tokenize(file.decode())\n",
    "            score = 0\n",
    "            class_count = {}\n",
    "            \n",
    "            # First function, we want to count the number of times we\n",
    "            # see a class being used\n",
    "            done = False\n",
    "            index = 0\n",
    "            while done == False and index < len(tokens):\n",
    "                token = tokens[index]\n",
    "                if token.lower() == \"cls\" and tokens[index-1] == '[' and tokens[index+1] == ']':\n",
    "                    done = True\n",
    "                if token in self.unique_classes:\n",
    "                    if token in class_count:\n",
    "                        class_count[token] += 1\n",
    "                    else:\n",
    "                        class_count[token] = 1\n",
    "                index += 1\n",
    "                        \n",
    "            # Here, we are on the 2nd function (usually)\n",
    "            # or this file doesn't have a 2nd function (unlikely).\n",
    "            # Now basically we want to see if the classes encountered here\n",
    "            # are also classes that haved used in the first function.\n",
    "            # Add 1 to similarity score for each class both functions\n",
    "            # are using\n",
    "            while index < len(tokens):\n",
    "                token = tokens[index]\n",
    "                if token in self.unique_classes and token in class_count and class_count[token] > 0:\n",
    "                    class_count[token] -= 1\n",
    "                    score += 1\n",
    "                index += 1\n",
    "                \n",
    "            retVal.append(score)\n",
    "        return pd.DataFrame(retVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8260 (+/- 0.0460)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = ColumnTransformer([\n",
    "    (\"body\", CountVectorizer(), \"body\"),\n",
    "    (\"classes\", SimilarityScorer(), \"classes\")\n",
    "])\n",
    "\n",
    "df = pd.DataFrame({\"body\": dataset.data, \"classes\": dataset.data})\n",
    "\n",
    "evaluate_cross_val_score(df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Counting the java keywords in each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_kwords = {\n",
    "    'abstract', 'continue', 'for', 'new',\n",
    "    'switch', 'assert', 'default', 'goto',\n",
    "    'package', 'synchronized' 'boolean', 'do',\n",
    "    'if', 'private', 'this','break', 'double',\n",
    "    'implements', 'protected', 'throw', 'byte', \n",
    "    'else', 'import', 'public', 'throws', 'case',\n",
    "    'enum', 'instanceof', 'return', 'transient',\n",
    "    'catch', 'extends', 'int', 'short', 'try', \n",
    "    'char', 'final', 'interface', 'static', 'void',\n",
    "    'class', 'finally', 'long', 'strictfp', 'while',\n",
    "    'volatile', 'const', 'float', 'native', 'super'\n",
    "}\n",
    "\n",
    "java_kword_vectorizer = CountVectorizer()\n",
    "java_kword_vectorizer.fit(java_kwords)\n",
    "\n",
    "def get_keyword_vectorizer(code):\n",
    "    features = list()\n",
    "    tokens = word_tokenize(code)\n",
    "    for token in tokens:\n",
    "        if token in java_kwords:\n",
    "            features.append(token)\n",
    "    return java_kword_vectorizer.transform([' '.join(features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_methods(docs):\n",
    "    for doc in docs:\n",
    "        document = doc.decode()\n",
    "        left, right = document.split('[CLS]')\n",
    "        yield left, right\n",
    "\n",
    "class SimilarityScorerKeywords(BaseEstimator):\n",
    "    def fit(self, data, unused):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        retVal = []\n",
    "        \n",
    "        for left, right in separate_methods(documents):\n",
    "            left_counter = get_keyword_vectorizer(left)            \n",
    "            right_counter = get_keyword_vectorizer(right)\n",
    "            score = cosine_similarity(left_counter.todense(), right_counter.todense())\n",
    "            retVal.append(score[0][0])\n",
    "            # this returns the count as an array\n",
    "            #score = np.hstack([left_counter.todense()[0], right_counter.todense()[0]])\n",
    "            #retVal.append(np.array(score.flatten())[0])\n",
    "        return pd.DataFrame(retVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7720 (+/- 0.0700)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Warning caused by dependency on numpy\n",
    "\n",
    "vectorizer = ColumnTransformer([\n",
    "    (\"body\", CountVectorizer(ngram_range=(1,2)), \"body\"),\n",
    "    (\"kwords\", SimilarityScorerKeywords(), \"kwords\"),\n",
    "])\n",
    "\n",
    "df = pd.DataFrame({\"body\": dataset.data, \"kwords\": dataset.data})\n",
    "\n",
    "evaluate_cross_val_score(df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Counting number of arguments and return type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = (\"^[ \\t]*(?:(?:public|private|protected|static|final|native\"\n",
    "         \"|synchronized|abstract|transient|@Override|@Test)+\\s+)+\"\n",
    "         \"[$_\\w<>\\[\\]\\s]*\\s+[\\$_\\w]+\\([^\\)]*\\)?\\s*\")\n",
    "non_kword_regex = \"^[ \\t]*[$_\\w<>\\[\\]\\s]*\\s+[\\$_\\w]+\\([^\\)]*\\)?\\s*\"\n",
    "def get_java_func_signature(code):\n",
    "    match = re.match(regex, code)\n",
    "    ret_val_idx = -2\n",
    "    if not match:\n",
    "        match = re.match(non_kword_regex, code)\n",
    "        ret_val_idx = -1\n",
    "        if not match:\n",
    "            return \"void\", []\n",
    "    # remove )\n",
    "    string = match.group(0).lstrip()[:-2]\n",
    "    signature, args = string.split(\"(\")\n",
    "    args = args.split(\", \")\n",
    "    signature = signature.split(\" \")\n",
    "    ret_value = signature[ret_val_idx]\n",
    "    return ret_value, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityScoreArgCount(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Evaluates based on the number of arguments that each method has.\n",
    "    The score is len(left_args) - len(right_args)\n",
    "    \"\"\"\n",
    "    def fit(self, data, unused):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        args_diff = []\n",
    "        for left, right in separate_methods(documents):\n",
    "            _, largs = get_java_func_signature(left)\n",
    "            _, rargs = get_java_func_signature(right)\n",
    "            args_diff.append(len(largs) - len(rargs))\n",
    "        return pd.DataFrame(args_diff)\n",
    "\n",
    "\n",
    "class SimilarityScoreRetVals(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Score based on the return type of each method.\n",
    "    \"\"\"\n",
    "    def fit(self, data, unused):\n",
    "        self.unique_classes = set()\n",
    "        for left, right in separate_methods(data):\n",
    "            left_ret_val, _ = get_java_func_signature(left)\n",
    "            self.unique_classes.add(left_ret_val)\n",
    "            right_ret_val, _ = get_java_func_signature(right)\n",
    "            self.unique_classes.add(right_ret_val)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        vals = []\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(self.unique_classes)\n",
    "        vocab = vectorizer.vocabulary_\n",
    "        for left, right in separate_methods(documents):\n",
    "            ret_type_l, _ = get_java_func_signature(left)\n",
    "            ret_type_r, _ = get_java_func_signature(right)\n",
    "            lret_id = vocab.get(ret_type_l.lower(), -1)\n",
    "            rret_id = vocab.get(ret_type_r.lower(), -1)\n",
    "            vals.append([lret_id, rret_id])\n",
    "        return pd.DataFrame(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7705 (+/- 0.0662)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = ColumnTransformer([\n",
    "    (\"body\", CountVectorizer(), \"body\"),\n",
    "    (\"argcount\", SimilarityScoreArgCount(), \"argcount\"),\n",
    "])\n",
    "\n",
    "df = pd.DataFrame({\"body\": dataset.data, \"argcount\": dataset.data})\n",
    "\n",
    "evaluate_cross_val_score(df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❌ Using w2v embeddings of words in the method names\n",
    "\n",
    "> Note: run the code under methods-w2v.ipynb to generate the embeddings and save them to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_method_embeddings = load_files('code-clone-method-embeddings-left', encoding=\"utf-8\")\n",
    "right_method_embeddings = load_files('code-clone-method-embeddings-right', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_pd_series(data, col_prefix):\n",
    "    df = pd.Series(data, dtype='str', name='left')\n",
    "    df = df.str.split(',', expand=True)\n",
    "    \n",
    "    col_count = df.shape[1]\n",
    "    col_names = dict(zip(range(col_count), [f'{col_prefix}{i}' for i in range(col_count)]))\n",
    "    df = df.rename(columns=col_names).astype('float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_method_series = populate_pd_series(left_method_embeddings.data, 'left_method')\n",
    "right_mehod_series = populate_pd_series(right_method_embeddings.data, 'right_method')\n",
    "\n",
    "body = pd.Series(dataset.data, name='body')\n",
    "\n",
    "df = pd.concat([body, left_method_series, right_mehod_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7590 (+/- 0.0595)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = ColumnTransformer([\n",
    "    (\"body\", CountVectorizer(), \"body\"),\n",
    "], remainder='passthrough')\n",
    "\n",
    "evaluate_cross_val_score(df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Using w2v embeddings for all java tokens in each method\n",
    "\n",
    "> Note: run the code under tokens-w2v.ipynb to generate the embeddings and save them to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_token_embeddings = load_files('code-clone-embeddings-left', encoding=\"utf-8\")\n",
    "right_token_embeddings = load_files('code-clone-embeddings-right', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_token_series = populate_pd_series(left_token_embeddings.data, 'left_tokens')\n",
    "right_token_series = populate_pd_series(right_token_embeddings.data, 'right_tokens')\n",
    "\n",
    "body = pd.Series(dataset.data, name='body')\n",
    "\n",
    "df = pd.concat([body, left_token_series, right_token_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7680 (+/- 0.0697)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = ColumnTransformer([\n",
    "    (\"body\", CountVectorizer(), \"body\"),\n",
    "], remainder='passthrough')\n",
    "\n",
    "evaluate_cross_val_score(df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Combining the good features 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8330 (+/- 0.0448)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = ColumnTransformer([\n",
    "    (\"body\", CountVectorizer(), \"body\"),\n",
    "    (\"argcount\", SimilarityScoreArgCount(), \"argcount\"),\n",
    "    (\"kwords\", SimilarityScorerKeywords(), \"kwords\"),\n",
    "    (\"classes\", SimilarityScorer(), \"classes\")\n",
    "    \n",
    "], remainder=\"passthrough\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"body\": dataset.data, \n",
    "    \"argcount\": dataset.data,\n",
    "    \"kwords\": dataset.data,\n",
    "    \"classes\": dataset.data,    \n",
    "})\n",
    "\n",
    "df = pd.concat([\n",
    "    df, \n",
    "    left_token_series,\n",
    "    right_token_series,\n",
    "], axis=1)\n",
    "\n",
    "evaluate_cross_val_score(df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
