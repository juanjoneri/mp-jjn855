{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE W382V MACHINE PROGRAMMING\n",
    "\n",
    "# Homework 1 - due Sunday July 9 2022 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) and ensure your code does not give ANY error. \n",
    "\n",
    "For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible in the form of **python comment** for partial credit.  **Please do NOT leave imcomplete code in your homework, please wrap them up in the comment.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students. However, all the code you write must be your own!\n",
    "\n",
    "Information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any person you discussed this assignment with:\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: the paradox of induction (15 points)\n",
    "\n",
    "Consider a statement whose truth is unknown. If we see many examples that are compatible with it, we are tempted to view the statement as more probable. Such reasoning is often referred to as _inductive inference_ (in a philosophical, rather than mathematical sense). Consider now the statement that \"all cows are white\". An equivalent statement is that \"everything that is not white is not a cow\". We then observe several black panthers. Our observations are clearly compatible with the statement, but do they make the hypothesis \"all cows are white\" more likely?\n",
    "\n",
    "To analyze such a situation, we consider a probabilistic model. Let us assume that there are two possible states of the world, which we model as complementary events:\n",
    "\n",
    "<center>$A$: all cows are white,\n",
    "    \n",
    "<center>$A^c$: 50% of all cows are white.\n",
    "\n",
    "Let $p$ be the prior probability $P(A)$ that all cows are white. We make an observation of a cow or a panther, with probability $q$ and $1-q$, respectively, independent of whether event $A$ occurs or not. Assume that $0<p<1, 0<q<1$, and that all panthers are black.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Given the event $B=$\\{a black panther was observed\\}, what is $P(A|B)$? Show your work (5pts)\n",
    "\n",
    "Using the conditional rule, we get:\n",
    "\n",
    "$P(A|B)$ = $\\frac{P(A\\cap B)}{P(B)}$\n",
    "\n",
    "The probability of observing a black panther is independent of whether event A occurs or not, therefore:\n",
    "\n",
    "$\\frac{P(A\\cap B)}{P(B)}$ = $\\frac{P(A)\\times P(B)}{P(B)}$ = $P(A)$ = $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Given the event $C=$\\{a white cow was observed\\}, what was $P(A|C)$? Show your work (5pts)\n",
    "\n",
    "Applying Baye's theorem we get:\n",
    "\n",
    "$P(A|C)$ = $\\frac{P(C | A) P(A)}{P(C)}$\n",
    "\n",
    "$P(C | A)$ describes the probability of observing a white cow given that all cows are white. Since all cows are white, then this should be equivalent to the general probability of observing a cow, therefore:\n",
    "\n",
    "- $P(C | A)$ = $q$\n",
    "\n",
    "$P(A)$ is known from the problem statement:\n",
    "\n",
    "- $P(A)$ = $p$\n",
    "\n",
    "Finally since $A$ and $A^c$ are mutually exclusive, $P(C)$ can be computed as $P(C \\cap A)$ + $P(C \\cap A^c)$\n",
    "\n",
    "Using the multiplication rule we get:\n",
    "\n",
    "- $P(C \\cap A)$ = $P(A) P(C | A)$ = $p q$\n",
    "\n",
    "To find $P(C \\cap A^c)$ we first need to find $P(C | A^c)$. In a world where 50% of cows are white, the probability of observing a white cow $P(C | A^c)$ is just $\\frac{1}{2}$ that of observing a cow in general. Therefore:\n",
    "\n",
    "- $P(C | A^c)$ = $\\frac{1}{2}q$\n",
    "\n",
    "Using the multiplication rule again, we get:\n",
    "\n",
    "- $P(C \\cap A^c)$ = $P(A^c) P(C | A^c)$ = $(1 - p) \\frac{1}{2}q$\n",
    "\n",
    "Putting all together:\n",
    "\n",
    "- $P(A|C)$ = $\\frac{P(C | A) P(A)}{P(C)}$ = $\\frac{P(C | A) P(A)}{P(C \\cap A) + P(C \\cap A^c)}$ = $\\frac{q p}{(p q) + (1 - p)(\\frac{1}{2}q)}$\n",
    "\n",
    "Using walfram alpha to simplify the fraction we find:\n",
    "\n",
    "$P(A|C)$ = $2 - \\frac{2}{p + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Which is larger? Explain the implication. (5pts)\n",
    "\n",
    "For the interval $0<p<1$, we get that $2 - \\frac{2}{p + 1} \\gt p$ (this can be seen graphically by plotting $f(x) = x$ and $g(x) = 2 - \\frac{2}{x + 1}$ and observing that $g(x)$ has larger values of $y$ for $0<x<1$).\n",
    "\n",
    "- $P(A|C) \\gt P(A|B)$\n",
    "\n",
    "The implication is that observing a black panther does not make the hypothesis \"all cows are white\" more likely and is not equivalent to observing a white cow. Observing a white cow however does make the hypothesis \"all cows are white\" slightly more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2:  log odds ratios (35 points)\n",
    "This exercise is an exploratory analysis of the Sentiment140 dataset. Sentiment140 combines 160K tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) and neutral (2).  _We will  **not** consider neutral tweets in this problem_. You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "In this problem, we will analyze how often a word tend to appear with a positive sentiment vs. a negative one. The metric we are going to use is  **log odds ratio**, that compares the conditional probability of a word occurring in one type of sentences, say, positive ($P(word|pos)$), and the word occurring in another type of sentences, say, negative ($P(word|neg)$):\n",
    "$$log\\_odds\\_ratio(word, pos) = \\log\\frac{P(word|pos)}{P(word|neg)}$$\n",
    "The higher the $log\\_odds\\_ratio$, the more likely the word is associated with positive sentences.\n",
    "\n",
    "\n",
    "Download from Canvas the file ``sentiment140_sample1.csv`` ---a 10K sample from the training set of Sentiment140---and put it under the  **same directory** (folder) as your python script or notebook file. As a reminder, the file is formatted under six fields, including polarity, tweet ID, date, query username and the text of the tweet. We will only use polarity and tweet text in this assignment.\n",
    "\n",
    "In the following exercises, we have provided several expected inputs and outputs of the functions that you will implement. Treat these as test cases for your code; if you get numbers very far off from what is listed here with the same input, you have bugs to crush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467817374</td>\n",
       "      <td>Mon Apr 06 22:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ajaxpro</td>\n",
       "      <td>@MissXu sorry! bed time came here (GMT+1)   ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467863716</td>\n",
       "      <td>Mon Apr 06 22:33:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stacyc37</td>\n",
       "      <td>sad that the 'feet' of my macbook just fell off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467878057</td>\n",
       "      <td>Mon Apr 06 22:37:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>debbieseraphina</td>\n",
       "      <td>help me forget 8th april &amp;amp; 13th july!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467909292</td>\n",
       "      <td>Mon Apr 06 22:45:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>satori</td>\n",
       "      <td>@soillodge yes, it will be. it's only Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1468045043</td>\n",
       "      <td>Mon Apr 06 23:25:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TigerHasse</td>\n",
       "      <td>Debbugging old VB6 code, the day could have st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467817374  Mon Apr 06 22:21:30 PDT 2009  NO_QUERY          ajaxpro   \n",
       "1  0  1467863716  Mon Apr 06 22:33:35 PDT 2009  NO_QUERY         stacyc37   \n",
       "2  0  1467878057  Mon Apr 06 22:37:26 PDT 2009  NO_QUERY  debbieseraphina   \n",
       "3  0  1467909292  Mon Apr 06 22:45:54 PDT 2009  NO_QUERY           satori   \n",
       "4  0  1468045043  Mon Apr 06 23:25:27 PDT 2009  NO_QUERY       TigerHasse   \n",
       "\n",
       "                                                   5  \n",
       "0  @MissXu sorry! bed time came here (GMT+1)   ht...  \n",
       "1   sad that the 'feet' of my macbook just fell off   \n",
       "2         help me forget 8th april &amp; 13th july!   \n",
       "3      @soillodge yes, it will be. it's only Monday   \n",
       "4  Debbugging old VB6 code, the day could have st...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data (DO NOT CHANGE)\n",
    "import pandas\n",
    "sentiment_data = pandas.read_csv(\"../datasets/sentiment140_sample1.csv\", header = None, encoding = \"ISO-8859-1\")\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.009200</td>\n",
       "      <td>1.997586e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000079</td>\n",
       "      <td>1.951833e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467817e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.956556e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.002018e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.176924e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.329177e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  10000.000000  1.000000e+04\n",
       "mean       2.009200  1.997586e+09\n",
       "std        2.000079  1.951833e+08\n",
       "min        0.000000  1.467817e+09\n",
       "25%        0.000000  1.956556e+09\n",
       "50%        4.000000  2.002018e+09\n",
       "75%        4.000000  2.176924e+09\n",
       "max        4.000000  2.329177e+09"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Frequency counts  (5 points)\n",
    "First, let's create dictionaries that record the count of each word in positive tweets, as well as the count of each word in negative tweets. Here, here, ``counts[\"pos\"]`` will contain key-value pairs of a word and its number of appearance in positive tweets, ``counts[\"neg\"]`` will contain key-value pairs of a word and its number of appearance in negative tweets\n",
    "\n",
    "To parse the tweets, we will use NLTK's ``word_tokenize()`` function. As an example, the following tokenizes a sentence into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jboy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #you only have to do this once per environment\n",
    "\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-casing all words gives cleaner counts. For example, consider the two sentences: \"Apples are delicious. John loves apples.\" If we do not lower-case each word, ''Apples'' and ''apples'' will be counted as two different words. In Python, you can lower-case a word by calling ``lower()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Apples becomes apples\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\" == \"apples\")\n",
    "print(\"Apples becomes\", \"Apples\".lower())\n",
    "print(\"Apples\".lower() == \"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider words and not symbols or numbers. To test whether a word is a word, that is, consisting of only English characters, we can use ``isalpha()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\".isalpha())\n",
    "print(\"Apples123\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_counts(data):\n",
    "    \"\"\" \n",
    "    counts the number of times a word appears in negative or positive tweets\n",
    "    \n",
    "    Parameters:\n",
    "    data: Pandas dataframe of tweets\n",
    "    \n",
    "    Returns:\n",
    "    counts: Dictionary of counts, which includes the dictionaries 'pos' and 'neg'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        sentiment = row[0]\n",
    "        tweet = row[5]\n",
    "        \n",
    "        for word in word_tokenize(tweet):\n",
    "            counts[sentiment][word.lower()] += word.isalpha()\n",
    "            \n",
    "        \n",
    "    \n",
    "    return {\"pos\": dict(counts[4]), \"neg\": dict(counts[0])} # positive (4), negative (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "38\n",
      "15\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# Do not change\n",
    "counts = get_counts(sentiment_data)\n",
    "print(counts[\"pos\"][\"happy\"]) # should print 124 or 127\n",
    "print(counts[\"neg\"][\"happy\"]) # should print 38\n",
    "print(counts[\"pos\"][\"hate\"]) # shuld print 15\n",
    "print(counts[\"neg\"][\"hate\"]) # should print 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculating $P(\\text{word}|\\text{polarity})$ (10 points)\n",
    "\n",
    "Create a function ``get_word_prob(counts, word, polarity)``, where ``counts`` is a dictionary like in the previous task, ``word`` is the word for which $P(word|polarity)$ will be calculated, and ``polarity`` is either ``pos`` or ``neg``. The function should return $P(word|polarity)$. If ``counts[polarity]`` does not contain ``word``, then return 0.\n",
    "\n",
    "Note that you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts, word, polarity``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_prob(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    calculates the probability of a word given a polarity \n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    probability (float):  the probability of a word given a polarity \n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    \n",
    "    universe_size = sum(count for _, count in counts[polarity].items()) # Number of total pos/neg words\n",
    "    try:\n",
    "        event_size = counts[polarity][word]\n",
    "    except KeyError:\n",
    "        event_size = 0\n",
    "    \n",
    "    probability = event_size / universe_size\n",
    "    \n",
    "    return probability # P(word|polarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002541681986028742\n",
      "0.00012071280913795966\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Do not change\n",
    "print(get_word_prob(counts, \"great\", \"pos\")) # should be ~0.00254\n",
    "print(get_word_prob(counts, \"glad\", \"neg\")) # should be ~0.000121\n",
    "print(get_word_prob(counts, \"wugs\", \"neg\")) # should be 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Calculate the log odds ratio of a word  (10 points)\n",
    "\n",
    "\n",
    "Using the above function, we can calculate $P(word|pos)$ and $P(word|neg)$ given a word, so we are ready to calculate the log odds ratio of that word as well. Create a function ``log_odds_ratio(count_dict, word, polarity)``, where the arguments are of the same type/format as in the previous problem. The function should return $log\\_odds\\_ratio(word)$:\n",
    "\n",
    "$$ log\\_odds\\_ratio(word, polarity) = \\log\\frac{P(word|polarity)}{P(word|opposite\\_polarity)} $$\n",
    "\n",
    "If the denominator is zero, return a very large number (eg 10000). Again you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts``, ``word``, and ``polarity``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import sys\n",
    "\n",
    "def log_odds_ratio(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    This function returns the log odds ratio of a term (see previous cell)\n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    log_odds_ratio (float): log( prob(word|plarity) / P(word|opposity_polarity) )\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    opposite_polarity = {'pos': 'neg', 'neg': 'pos'}\n",
    "    \n",
    "    p_wp = get_word_prob(counts, word, polarity)\n",
    "    p_wop = get_word_prob(counts, word, opposite_polarity[polarity])\n",
    "    \n",
    "    try:\n",
    "        log_odds_ratio = log(p_wp / p_wop)\n",
    "    except ZeroDivisionError:\n",
    "        # division by 0\n",
    "        log_odds_ratio = 10000\n",
    "    except ValueError:\n",
    "        # log of 0\n",
    "        log_odds_ratio = 0\n",
    "        \n",
    "    return log_odds_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2764610338152973\n",
      "-0.0905923001499596\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Do not change\n",
    "print(log_odds_ratio(counts, \"great\", \"pos\")) # should be ~1.276\n",
    "print(log_odds_ratio(counts, \"the\", \"neg\")) #  should be ~-0.0906\n",
    "print(log_odds_ratio(counts, \"wug\", \"neg\")) # should be a very large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Sorting log odds ratios (10 points)\n",
    "\n",
    "After being able to calculate log odds ratios for individual words, we can now sort words according to its association with a polarity class, say, positive. Create a function ``sort_pos_words(data)``, that takes in the entire dataframe as an argument, and return a sorted list of ``(word, log odds ratio)`` tuples for the positive sentiment class.\n",
    "\n",
    "If you implement this without filtering out any words, you will notice that there are many cases where the conditional probability of the denominator is 0, leading to the very large number you specified in the ``log_odds_ratio()`` function. This is because most words appear only once in the dataset. One way to mitigate this issue is to consider only words that appeared at least $x$ times in the dataset; here, let's only include words that appeared more than 10 times in the dataset, regardless of the polarity of the tweet (positive or negative).\n",
    "\n",
    "Use your function to print out the top 10 most positive:\n",
    "\n",
    "`` [('proud', 10000), ('congratulations', 10000), ('vip', 2.7657670338765064), ('yum', 2.696774162389555), ('worry', 2.455612105572667), ('mothers', 2.455612105572667), ('thank', 2.393091748591333), ('jonasbrothers', 2.360301925768342), ('sir', 2.360301925768342), ('fabulous', 2.360301925768342)]``\n",
    "       \n",
    " and the top 10  most negative:\n",
    " \n",
    "`` [('expensive', -2.508004655425329), ('bus', -2.5821126275790505), ('throat', -2.651105499066002), ('hates', -2.651105499066002), ('tummy', -2.715644020203573), ('sad', -2.8052561788932606), ('missing', -2.987577735687215), ('died', -3.121109128311738), ('headache', -3.4694158225799536), ('hurts', -3.8348755960744185)]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pos_words(data, word_appearance_min=10):\n",
    "    \"\"\"\n",
    "    takes in a pandas dataframe and outputs the top 10 most positive and negative words in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): the tweets in a dataframe\n",
    "    \n",
    "    Return:\n",
    "    sorted_list (list): a sorted list of (word, log odds ratio) tuples for the positive sentiment class\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    counts = get_counts(data)\n",
    "    \n",
    "    # Find a set of all the popular words (those that appear at least word_appearance_min times)\n",
    "    words = defaultdict(int)\n",
    "    for polarity in ('pos', 'neg'):\n",
    "        for word, count in counts[polarity].items():\n",
    "            words[word] += count\n",
    "\n",
    "    popular_words = {word for word, count in words.items() if count >= word_appearance_min}\n",
    "    \n",
    "    # Lambda function to compute the positive log odds ratio of a word\n",
    "    pos_log_odds_ratio = lambda word: log_odds_ratio(counts, word, 'pos')\n",
    "    \n",
    "    # Compute the positive log odds ratios of the popular words\n",
    "    pos_log_odds_ratios = [(word, pos_log_odds_ratio(word)) for word in popular_words]\n",
    "    \n",
    "    # Sort the list by decreasing log odd ratio\n",
    "    pos_log_odds_ratios.sort(reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    return pos_log_odds_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most positive \n",
      " [('congratulations', 10000), ('proud', 10000), ('vip', 2.7657546344073345), ('yum', 2.696761762920383), ('mothers', 2.455599706103495), ('worry', 2.455599706103495), ('thank', 2.393079349122161), ('sir', 2.36028952629917), ('jonasbrothers', 2.36028952629917), ('fabulous', 2.36028952629917)]\n",
      "Top 10 most negative \n",
      " [('expensive', -2.5072449241564128), ('bus', -2.5813528963101344), ('hates', -2.6503457677970856), ('throat', -2.714884288934657), ('tummy', -2.714884288934657), ('sad', -2.7373571447867153), ('missing', -2.9868180044182986), ('died', -3.1203493970428213), ('headache', -3.4976436281842895), ('hurts', -3.8741211994192013)]\n"
     ]
    }
   ],
   "source": [
    "# Do not change\n",
    "lst = sort_pos_words(sentiment_data)\n",
    "print(\"Top 10 most positive \\n\", lst[:10]) # see previous cell for what this should print\n",
    "print(\"Top 10 most negative \\n\", lst[-10:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
