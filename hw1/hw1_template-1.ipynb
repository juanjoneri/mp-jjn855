{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE W382V MACHINE PROGRAMMING\n",
    "\n",
    "# Homework 1 - due Sunday July 9 2022 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) and ensure your code does not give ANY error. \n",
    "\n",
    "For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible in the form of **python comment** for partial credit.  **Please do NOT leave imcomplete code in your homework, please wrap them up in the comment.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students. However, all the code you write must be your own!\n",
    "\n",
    "Information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any person you discussed this assignment with:\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: the paradox of induction (15 points)\n",
    "\n",
    "Consider a statement whose truth is unknown. If we see many examples that are compatible with it, we are tempted to view the statement as more probable. Such reasoning is often referred to as _inductive inference_ (in a philosophical, rather than mathematical sense). Consider now the statement that \"all cows are white\". An equivalent statement is that \"everything that is not white is not a cow\". We then observe several black panthers. Our observations are clearly compatible with the statement, but do they make the hypothesis \"all cows are white\" more likely?\n",
    "\n",
    "To analyze such a situation, we consider a probabilistic model. Let us assume that there are two possible states of the world, which we model as complementary events:\n",
    "\n",
    "<center>$A$: all cows are white,\n",
    "    \n",
    "<center>$A^c$: 50% of all cows are white.\n",
    "\n",
    "Let $p$ be the prior probability $P(A)$ that all cows are white. We make an observation of a cow or a panther, with probability $q$ and $1-q$, respectively, independent of whether event $A$ occurs or not. Assume that $0<p<1, 0<q<1$, and that all panthers are black.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Given the event $B=$\\{a black panther was observed\\}, what is $P(A|B)$? Show your work (5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Given the event $C=$\\{a white cow was observed\\}, what was $P(A|C)$? Show your work (5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Which is larger? Explain the implication. (5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2:  log odds ratios (35 points)\n",
    "This exercise is an exploratory analysis of the Sentiment140 dataset. Sentiment140 combines 160K tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) and neutral (2).  _We will  **not** consider neutral tweets in this problem_. You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "In this problem, we will analyze how often a word tend to appear with a positive sentiment vs. a negative one. The metric we are going to use is  **log odds ratio**, that compares the conditional probability of a word occurring in one type of sentences, say, positive ($P(word|pos)$), and the word occurring in another type of sentences, say, negative ($P(word|neg)$):\n",
    "$$log\\_odds\\_ratio(word, pos) = \\log\\frac{P(word|pos)}{P(word|neg)}$$\n",
    "The higher the $log\\_odds\\_ratio$, the more likely the word is associated with positive sentences.\n",
    "\n",
    "\n",
    "Download from Canvas the file ``sentiment140_sample1.csv`` ---a 10K sample from the training set of Sentiment140---and put it under the  **same directory** (folder) as your python script or notebook file. As a reminder, the file is formatted under six fields, including polarity, tweet ID, date, query username and the text of the tweet. We will only use polarity and tweet text in this assignment.\n",
    "\n",
    "In the following exercises, we have provided several expected inputs and outputs of the functions that you will implement. Treat these as test cases for your code; if you get numbers very far off from what is listed here with the same input, you have bugs to crush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data (DO NOT CHANGE)\n",
    "import pandas\n",
    "sentiment_data = pandas.read_csv(\"sentiment140_sample1.csv\", header = None, encoding = \"ISO-8859-1\")\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Frequency counts  (5 points)\n",
    "First, let's create dictionaries that record the count of each word in positive tweets, as well as the count of each word in negative tweets. Here, here, ``counts[\"pos\"]`` will contain key-value pairs of a word and its number of appearance in positive tweets, ``counts[\"neg\"]`` will contain key-value pairs of a word and its number of appearance in negative tweets\n",
    "\n",
    "To parse the tweets, we will use NLTK's ``word_tokenize()`` function. As an example, the following tokenizes a sentence into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #you only have to do this once per environment\n",
    "\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-casing all words gives cleaner counts. For example, consider the two sentences: \"Apples are delicious. John loves apples.\" If we do not lower-case each word, ''Apples'' and ''apples'' will be counted as two different words. In Python, you can lower-case a word by calling ``lower()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Apples\" == \"apples\")\n",
    "print(\"Apples becomes\", \"Apples\".lower())\n",
    "print(\"Apples\".lower() == \"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider words and not symbols or numbers. To test whether a word is a word, that is, consisting of only English characters, we can use ``isalpha()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Apples\".isalpha())\n",
    "print(\"Apples123\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(data):\n",
    "    \"\"\" \n",
    "    counts the number of times a word appears in negative or positive tweets\n",
    "    \n",
    "    Parameters:\n",
    "    data: Pandas dataframe of tweets\n",
    "    \n",
    "    Returns:\n",
    "    counts: Dictionary of counts, which includes the dictionaries 'pos' and 'neg'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {\"pos\":{}, \"neg\":{}}\n",
    "    # Your code goes here\n",
    "    \n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change\n",
    "counts = get_counts(sentiment_data)\n",
    "print(counts[\"pos\"][\"happy\"]) # should print 124 or 127\n",
    "print(counts[\"neg\"][\"happy\"]) # should print 38\n",
    "print(counts[\"pos\"][\"hate\"]) # shuld print 15\n",
    "print(counts[\"neg\"][\"hate\"]) # should print 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculating $P(\\text{word}|\\text{polarity})$ (10 points)\n",
    "\n",
    "Create a function ``get_word_prob(counts, word, polarity)``, where ``counts`` is a dictionary like in the previous task, ``word`` is the word for which $P(word|polarity)$ will be calculated, and ``polarity`` is either ``pos`` or ``neg``. The function should return $P(word|polarity)$. If ``counts[polarity]`` does not contain ``word``, then return 0.\n",
    "\n",
    "Note that you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts, word, polarity``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_prob(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    calculates the probability of a word given a polarity \n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    probability (float):  the probability of a word given a polarity \n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    \n",
    "    \n",
    "    return probability # P(word|polarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not change\n",
    "print(get_word_prob(counts, \"great\", \"pos\")) # should be ~0.00254\n",
    "print(get_word_prob(counts, \"glad\", \"neg\")) # should be ~0.000121\n",
    "print(get_word_prob(counts, \"wugs\", \"neg\")) # should be 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Calculate the log odds ratio of a word  (10 points)\n",
    "\n",
    "\n",
    "Using the above function, we can calculate $P(word|pos)$ and $P(word|neg)$ given a word, so we are ready to calculate the log odds ratio of that word as well. Create a function ``log_odds_ratio(count_dict, word, polarity)``, where the arguments are of the same type/format as in the previous problem. The function should return $log\\_odds\\_ratio(word)$:\n",
    "\n",
    "$$ log\\_odds\\_ratio(word, polarity) = \\log\\frac{P(word|polarity)}{P(word|opposite\\_polarity)} $$\n",
    "\n",
    "If the denominator is zero, return a very large number (eg 10000). Again you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts``, ``word``, and ``polarity``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_odds_ratio(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    This function returns the log odds ratio of a term (see previous cell)\n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    log_odds_ratio (float): log( prob(word|plarity) / P(word|opposity_polarity) )\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    \n",
    "    return log_odds_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change\n",
    "print(log_odds_ratio(counts, \"great\", \"pos\")) # should be ~1.276\n",
    "print(log_odds_ratio(counts, \"the\", \"neg\")) #  should be ~-0.0906\n",
    "print(log_odds_ratio(counts, \"wug\", \"neg\")) # should be a very large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Sorting log odds ratios (10 points)\n",
    "\n",
    "After being able to calculate log odds ratios for individual words, we can now sort words according to its association with a polarity class, say, positive. Create a function ``sort_pos_words(data)``, that takes in the entire dataframe as an argument, and return a sorted list of ``(word, log odds ratio)`` tuples for the positive sentiment class.\n",
    "\n",
    "If you implement this without filtering out any words, you will notice that there are many cases where the conditional probability of the denominator is 0, leading to the very large number you specified in the ``log_odds_ratio()`` function. This is because most words appear only once in the dataset. One way to mitigate this issue is to consider only words that appeared at least $x$ times in the dataset; here, let's only include words that appeared more than 10 times in the dataset, regardless of the polarity of the tweet (positive or negative).\n",
    "\n",
    "Use your function to print out the top 10 most positive:\n",
    "\n",
    "`` [('proud', 10000), ('congratulations', 10000), ('vip', 2.7657670338765064), ('yum', 2.696774162389555), ('worry', 2.455612105572667), ('mothers', 2.455612105572667), ('thank', 2.393091748591333), ('jonasbrothers', 2.360301925768342), ('sir', 2.360301925768342), ('fabulous', 2.360301925768342)]``\n",
    "       \n",
    " and the top 10  most negative:\n",
    " \n",
    "`` [('expensive', -2.508004655425329), ('bus', -2.5821126275790505), ('throat', -2.651105499066002), ('hates', -2.651105499066002), ('tummy', -2.715644020203573), ('sad', -2.8052561788932606), ('missing', -2.987577735687215), ('died', -3.121109128311738), ('headache', -3.4694158225799536), ('hurts', -3.8348755960744185)]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pos_words(data):\n",
    "    \"\"\"\n",
    "    takes in a pandas dataframe and outputs the top 10 most positive and negative words in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): the tweets in a dataframe\n",
    "    \n",
    "    Return:\n",
    "    sorted_list (list): a sorted list of (word, log odds ratio) tuples for the positive sentiment class\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here    \n",
    "    \n",
    "    return sorted_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change\n",
    "lst = sort_pos_words(sentiment_data)\n",
    "print(\"Top 10 most positive \\n\", lst[:10]) # see previous cell for what this should print\n",
    "print(\"Top 10 most negative \\n\", lst[-10:])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
