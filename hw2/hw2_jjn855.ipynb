{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-737ecffa804edefe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ECE W382V MACHINE PROGRAMMING\n",
    "\n",
    "# Homework 2 - due Wednesday July 27 2022 at 7:00am\n",
    "\n",
    "For this homework you will hand in (upload) to canvas your code named ``hw2_YourEID.ipynb``.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (Kernel >> Restart and Run All) and ensure your code does __NOT__ give ANY error.\n",
    "\n",
    "For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible in the form of __python comment__ for partial credit. Please do NOT leave imcomplete code in your homework, please wrap them up in the comment.\n",
    "\n",
    "For non-programming tasks, make sure the cell is a markdown cell. Information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students. However, all the code you write must be your own!\n",
    "\n",
    "\n",
    "\n",
    "### Please list any person you discussed this assignment with:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e27014b61dce338",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 1: Naive Bayes (15 points)\n",
    "\n",
    "We'll do a bit of manual parameter estimation here.\n",
    "\n",
    "## **(a)** (8 points) \n",
    "Calculate the sufficient parameters for Naive Bayes using the data in the figure below, that\n",
    "is, the prior class probabilities and the conditional probabilities for all of the symbols.\n",
    "\n",
    "doc | X | Y\n",
    "-- | --| --\n",
    "d1 | a b b b c b | +\n",
    "d2 | c a c c c b | -\n",
    "d3 | c c c b | -\n",
    "d4 | b a b b b | +\n",
    "d5 | b c a b b | ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Prior Class Probabilities:\n",
    "- $P(+) = \\frac{1}{2}$\n",
    "- $P(-) = \\frac{1}{2}$\n",
    "\n",
    "Conditional Symbol Probabilities:\n",
    "- $P(a|+) = \\frac{2}{11}$\n",
    "- $P(a|-) = \\frac{1}{10}$\n",
    "---\n",
    "- $P(b|+) = \\frac{8}{11}$\n",
    "- $P(b|-) = \\frac{2}{10}$\n",
    "---\n",
    "- $P(c|+) = \\frac{1}{11}$\n",
    "- $P(c|-) = \\frac{7}{10}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(b)** (7 points) \n",
    "Using these, calculate the most likely class (pos or neg) for the unlabeled example (d5, labeled ???)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "- $P(+|b, c, a, b, b)$ = $P(+) P(b|+)^3 P(c|+) P(a|+)$ = $\\frac{1}{2} * (\\frac{8}{11})^3 * \\frac{1}{11} * \\frac{2}{11}$ = 0.00318\n",
    "---\n",
    "- $P(-|b, c, a, b, b)$ = $P(-) P(b|-)^3 P(c|-) P(a|-)$ = $\\frac{1}{2} * (\\frac{2}{10})^3 * \\frac{7}{10} * \\frac{1}{10}$ = 0.00028\n",
    "---\n",
    "\\+  is the most likely label for d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (logs of the fractions above)\n",
      " [-0.69314718 -0.69314718] \n",
      "\n",
      "Features (logs of the fractions above)\n",
      " [[-1.70474809 -0.31845373 -2.39789527]\n",
      " [-2.30258509 -1.60943791 -0.35667494]] \n",
      "\n",
      "Prediction:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jboy/.local/lib/python3.8/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['+'], dtype='<U1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "X = [\n",
    "    [1, 4, 1],\n",
    "    [1, 1, 4],\n",
    "    [0, 1, 3],\n",
    "    [1, 4, 0],\n",
    "]\n",
    "X = np.transpose(X)\n",
    "y = ['+', '-','-','+']\n",
    "\n",
    "clf = MultinomialNB(alpha=0)\n",
    "clf.fit(np.transpose(X), y)\n",
    "\n",
    "print(\"Classes (logs of the fractions above)\\n\", clf.class_log_prior_, \"\\n\")\n",
    "print(\"Features (logs of the fractions above)\\n\", clf.feature_log_prob_, \"\\n\")\n",
    "\n",
    "print(\"Prediction:\")\n",
    "clf.predict([[1, 3, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Language Modeling (85 points)\n",
    "\n",
    "In this problem, we will implement our own Java language models!\n",
    "\n",
    "### Data\n",
    "\n",
    "We will be working with a few different files under the `data` subdirectory. \n",
    "- `apache_spark.large.txt`\n",
    "- `apache_spark.short.txt`\n",
    "- `google_gson.large.txt`\n",
    "- `test.java.txt`\n",
    "\n",
    "You can use the \"short\" text files to do testing and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Find n-grams. (10 points) \n",
    "\n",
    "Create a function `make_ngram_tuples(words, n)` that returns a sequence of all n-grams seen in the input, in order.  The function returns a sequence of tuples where each tuple is of the form `(context, word)`.  The context is a tuple of the preceding n−1 words for each word; if the number of preceding words is smaller than n−1, place a `<s>` symbol in place of each missing context.  Close each sentence with an additional token `</s>`.  You can assumen n>1, that is, we are interested in enumerating bigrams, trigrams etc, and not unigrams.\n",
    "\n",
    "For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> samples = [’she’, ’eats’, ’happily’ ’.’]\n",
    ">>> make_ngram_tuples(samples, 2)\n",
    "[((’<s>’,), ’she’), ((’she’,), ’eats’), ((’eats’,), ’happily’), ((’happily’,), ’.’), ((’.’,), ’</s>’)]\n",
    ">>> make_ngram_tuples(samples, 3)\n",
    "[((’<s>’, ’<s>’), ’she’), ((’<s>’, ’she’), ’eats’), ((’she’, ’eats’), ’happily’),((’eats’, ’happily’), ’.’), ((’happily’, ’.’), ’</s>’)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple  # Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def make_ngram_tuples(words, n) -> List[Tuple]:\n",
    "    tuples = []\n",
    "    for t in zip(*[['<s>'] * i + words + ['</s>'] for i in reversed(range(n))]):\n",
    "        tuples.append((t[:n-1], t[n-1]))\n",
    "        \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<s>', '<s>', '<s>', '<s>'), 'she'),\n",
       " (('<s>', '<s>', '<s>', 'she'), 'eats'),\n",
       " (('<s>', '<s>', 'she', 'eats'), 'happliy'),\n",
       " (('<s>', 'she', 'eats', 'happliy'), '.'),\n",
       " (('she', 'eats', 'happliy', '.'), '</s>')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change\n",
    "samples = [\"she\", \"eats\", \"happliy\", \".\"]\n",
    "make_ngram_tuples(samples, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b)  Building an n-gram language model\n",
    "\n",
    "We are now ready to estimate a bigram language model from the training data.  We will use **Laplace smoothing (add-1)**.\n",
    "\n",
    "### (b1) Process the training file (8 points)\n",
    "\n",
    "We will first need to transform a file of tokenized functions into a list of \"function\", where each \"function\" is a list of *lower-cased* tokens. Since we have already provided the tokenized functions (see files in the `data/` directory), you only need to split the given string by space. Implement a function `process_code(codefile)` to do so. Assume that the path of `codefile` is of a form like `data/apache_spark.short.txt`, i.e., relative paths. \n",
    "\n",
    "```\n",
    ">>> tokenized_funcs = process_code(\"data/apache_spark.short.txt\")\n",
    ">>> tokenized_funcs[1]\n",
    "['public', 'file', 'get', 'file', '(', ')', '{', 'return', 'file', ';', '}']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_code(codefile) -> List[List[str]]:\n",
    "    return [l.split() for l in open(codefile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public', 'file', 'get', 'file', '(', ')', '{', 'return', 'file', ';', '}']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change\n",
    "tokenized_funcs = process_code(\"../datasets/hw2/apache_spark.short.txt\")\n",
    "tokenized_funcs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b2) Vocabulary (8 points).\n",
    "\n",
    "The language model ought to be able to handle words not seen in the training data.  Such words will most certainly appear if the LM is used in some application to estimate the likelihood of new data.   There  are  many  ways  to  incorporate  unknown  vocabulary  in  a  language  model.   In  this problem, we will take all words that appear only once and replace them with the symbol `<UNK>`. Thus, at test time, if we encounter an out-of-vocabulary word, we can resort to replacing the word with `<UNK>` which has well-formed probabilities.\n",
    "\n",
    "First, implement a function `get_vocab(tokenized_funcs)` where the parameter `tokenized_funcs` is a list of tokenized \"function\" (where each \"function\" is a list of lower-cased tokens) returned by the function `process_code`. The function `get_vocab` will return a `set` of all unique words in `tokenized_funcs` that appeared **more than** once.\n",
    "\n",
    "```\n",
    ">>> vocab = get_vocab(tokenized_funcs)\n",
    ">>> len(vocab)\n",
    "1375\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocab(tokenized_funcs) -> set:\n",
    "    counter = Counter(token for function in tokenized_funcs for token in function)\n",
    "    return {token for token, count in counter.items() if count > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change\n",
    "vocab = get_vocab(tokenized_funcs)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b3) Process unknown words (8 points).\n",
    "\n",
    "Write a function `process_unk(tokenized_funcs, vocab)` that takes in (1) tokenized functions returned by `process_code`, and (2) a vocabulary (set of words) returned by `get_vocab`. The function returns a list of tokenized functions that is the same as `tokenized_funcs` except that all words appearing only once will be replaced by the symbol `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> processed_funcs[3]\n",
    "['@', 'override', 'public', 'operation', 'handle', 'get', 'table', 'types', '(', ')', 'throws', 'hive', 'sqlexception', '{', 'acquire', '(', 'true', ')', ';', 'operation', 'manager', 'operation', 'manager', '=', 'get', 'operation', 'manager', '(', ')', ';', 'get', 'table', 'types', 'operation', 'operation', '=', 'operation', 'manager', '.', 'new', 'get', 'table', 'types', 'operation', '(', 'get', 'session', '(', ')', ')', ';', 'operation', 'handle', 'op', 'handle', '=', 'operation', '.', 'get', 'handle', '(', ')', ';', 'try', '{', 'operation', '.', 'run', '(', ')', ';', 'op', 'handle', 'set', '.', 'add', '(', 'op', 'handle', ')', ';', 'return', 'op', 'handle', ';', '}', 'catch', '(', 'hive', 'sqlexception', 'e', ')', '{', 'operation', 'manager', '.', 'close', 'operation', '(', 'op', 'handle', ')', ';', 'throw', 'e', ';', '}', 'finally', '{', 'release', '(', 'true', ')', ';', '}', '}']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unk(tokenized_funcs, vocab) -> List[List[str]]:\n",
    "    replace_unk = lambda func: ['<UNK>' if token not in vocab else token for token in func]\n",
    "    return [replace_unk(func) for func in tokenized_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'override',\n",
       " 'public',\n",
       " 'operation',\n",
       " 'handle',\n",
       " 'get',\n",
       " 'table',\n",
       " 'types',\n",
       " '(',\n",
       " ')',\n",
       " 'throws',\n",
       " 'hive',\n",
       " 'sqlexception',\n",
       " '{',\n",
       " 'acquire',\n",
       " '(',\n",
       " 'true',\n",
       " ')',\n",
       " ';',\n",
       " 'operation',\n",
       " 'manager',\n",
       " 'operation',\n",
       " 'manager',\n",
       " '=',\n",
       " 'get',\n",
       " 'operation',\n",
       " 'manager',\n",
       " '(',\n",
       " ')',\n",
       " ';',\n",
       " 'get',\n",
       " 'table',\n",
       " 'types',\n",
       " 'operation',\n",
       " 'operation',\n",
       " '=',\n",
       " 'operation',\n",
       " 'manager',\n",
       " '.',\n",
       " 'new',\n",
       " 'get',\n",
       " 'table',\n",
       " 'types',\n",
       " 'operation',\n",
       " '(',\n",
       " 'get',\n",
       " 'session',\n",
       " '(',\n",
       " ')',\n",
       " ')',\n",
       " ';',\n",
       " 'operation',\n",
       " 'handle',\n",
       " 'op',\n",
       " 'handle',\n",
       " '=',\n",
       " 'operation',\n",
       " '.',\n",
       " 'get',\n",
       " 'handle',\n",
       " '(',\n",
       " ')',\n",
       " ';',\n",
       " 'try',\n",
       " '{',\n",
       " 'operation',\n",
       " '.',\n",
       " 'run',\n",
       " '(',\n",
       " ')',\n",
       " ';',\n",
       " 'op',\n",
       " 'handle',\n",
       " 'set',\n",
       " '.',\n",
       " 'add',\n",
       " '(',\n",
       " 'op',\n",
       " 'handle',\n",
       " ')',\n",
       " ';',\n",
       " 'return',\n",
       " 'op',\n",
       " 'handle',\n",
       " ';',\n",
       " '}',\n",
       " 'catch',\n",
       " '(',\n",
       " 'hive',\n",
       " 'sqlexception',\n",
       " 'e',\n",
       " ')',\n",
       " '{',\n",
       " 'operation',\n",
       " 'manager',\n",
       " '.',\n",
       " 'close',\n",
       " 'operation',\n",
       " '(',\n",
       " 'op',\n",
       " 'handle',\n",
       " ')',\n",
       " ';',\n",
       " 'throw',\n",
       " 'e',\n",
       " ';',\n",
       " '}',\n",
       " 'finally',\n",
       " '{',\n",
       " 'release',\n",
       " '(',\n",
       " 'true',\n",
       " ')',\n",
       " ';',\n",
       " '}',\n",
       " '}']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change\n",
    "processed_unk = process_unk(tokenized_funcs, vocab)\n",
    "processed_unk[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['throw', '<UNK>', 'true']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = process_unk([[\"throw\", \"HOLA\", \"true\"]], vocab)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b4) N-gram frequencies (10 points).\n",
    "\n",
    "We now need to get the frequency counts -- which will allow us to calculate the conditional frequency distribution for our language model. Write a function `get_freq_dist(tokenized_funcs, n)` that takes in (1) a list of tokenized functions (such as one returned by `process_unk`), and (2) the number `n` that denotes the order of the n-gram (`n=2` means bigram, `n=3` means trigram, etc). The function will return a dictionary `freqdict` (`freqdict` can be a `Counter`) such that `freqdict[context][token]` records the number of times `token` follows `context`. You can think of `context` as the first element of a tuple in a list returned by `make_ngram_tuples`, and `token` as the second element of that tuple.\n",
    "\n",
    "```\n",
    ">>> freqdict = get_freq_dict(processed_unk, 2)\n",
    ">>> freqdict[('public',)]\n",
    "Counter({'void': 285, 'static': 70, 'boolean': 57, 'int': 39, 'string': 33, 'abstract': 16, 'long': 15, 'final': 13, 'get': 13, 'tget': 12, 'object': 8, 'org': 7, '_': 7, 'java': 7, 'short': 6, 'unsafe': 6, 'synchronized': 6, 'utf8': 6, 'table': 5, 'operation': 4, 'double': 4, 'float': 4, 'byte': 4, 'tclose': 4, 'texecute': 4, '<': 3, 'managed': 3, 'stream': 3, 'spark': 3, 'topen': 3, 'cancel': 3, 't': 3, 'close': 3, 'columnar': 3, 'optional': 2, 'row': 2, 'tstring': 2, 'map': 2, 'ttype': 2, 'tfetch': 2, 'fetch': 2, 'tcolumn': 2, 'ti16': 2, 'tstatus': 2, 'channel': 2, 'list': 2, 'circular': 2, 'tcancel': 2, 'calendar': 2, 'scan': 2, 'iterator': 2, 'file': 1, 'tmap': 1, '<UNK>': 1, 'transport': 1, 'avro': 1, 'server': 1, 'bloom': 1, 'tunion': 1, 'struct': 1, 'id': 1, 'mutable': 1, 'set': 1, 'tbool': 1, 'metadata': 1, 'parquet': 1, 'toperation': 1, 'tbyte': 1, 'integer': 1, 'tsession': 1, 'renew': 1, 'shuffle': 1, 'tstruct': 1, 'rpc': 1, 'trenew': 1, 'param': 1, 'decimal': 1, 'tprotocol': 1, 'metric': 1, 'nesting3': 1, 'location': 1, 'message': 1, '(': 1, 'execute': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_freq_dict(tokenized_funcs, n) -> dict:\n",
    "    vocab = get_vocab(tokenized_funcs)\n",
    "    tokenized_funcs_with_unk = process_unk(tokenized_funcs, vocab)\n",
    "    \n",
    "    freqdict = defaultdict(Counter)\n",
    "    \n",
    "    for func in tokenized_funcs_with_unk:\n",
    "        for context, token in make_ngram_tuples(func, n):\n",
    "            freqdict[context][token] += 1\n",
    "    \n",
    "    return freqdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'void': 285,\n",
       "         'file': 1,\n",
       "         'string': 33,\n",
       "         'operation': 4,\n",
       "         'org': 7,\n",
       "         'boolean': 57,\n",
       "         'static': 70,\n",
       "         '_': 7,\n",
       "         'double': 4,\n",
       "         'float': 4,\n",
       "         '<': 3,\n",
       "         'managed': 3,\n",
       "         'stream': 3,\n",
       "         'optional': 2,\n",
       "         'spark': 3,\n",
       "         'tmap': 1,\n",
       "         'int': 39,\n",
       "         'topen': 3,\n",
       "         '<UNK>': 1,\n",
       "         'final': 13,\n",
       "         'short': 6,\n",
       "         'unsafe': 6,\n",
       "         'synchronized': 6,\n",
       "         'row': 2,\n",
       "         'byte': 4,\n",
       "         'transport': 1,\n",
       "         'long': 15,\n",
       "         'avro': 1,\n",
       "         'abstract': 16,\n",
       "         'tget': 12,\n",
       "         'object': 8,\n",
       "         'cancel': 3,\n",
       "         't': 3,\n",
       "         'java': 7,\n",
       "         'tstring': 2,\n",
       "         'utf8': 6,\n",
       "         'tclose': 4,\n",
       "         'server': 1,\n",
       "         'map': 2,\n",
       "         'table': 5,\n",
       "         'bloom': 1,\n",
       "         'ttype': 2,\n",
       "         'get': 13,\n",
       "         'tfetch': 2,\n",
       "         'tunion': 1,\n",
       "         'struct': 1,\n",
       "         'id': 1,\n",
       "         'fetch': 2,\n",
       "         'mutable': 1,\n",
       "         'set': 1,\n",
       "         'tbool': 1,\n",
       "         'metadata': 1,\n",
       "         'texecute': 4,\n",
       "         'close': 3,\n",
       "         'parquet': 1,\n",
       "         'tcolumn': 2,\n",
       "         'toperation': 1,\n",
       "         'ti16': 2,\n",
       "         'tbyte': 1,\n",
       "         'tstatus': 2,\n",
       "         'channel': 2,\n",
       "         'list': 2,\n",
       "         'circular': 2,\n",
       "         'tcancel': 2,\n",
       "         'integer': 1,\n",
       "         'calendar': 2,\n",
       "         'columnar': 3,\n",
       "         'scan': 2,\n",
       "         'tsession': 1,\n",
       "         'renew': 1,\n",
       "         'iterator': 2,\n",
       "         'shuffle': 1,\n",
       "         'tstruct': 1,\n",
       "         'rpc': 1,\n",
       "         'trenew': 1,\n",
       "         'param': 1,\n",
       "         'decimal': 1,\n",
       "         'tprotocol': 1,\n",
       "         'metric': 1,\n",
       "         'nesting3': 1,\n",
       "         'location': 1,\n",
       "         'message': 1,\n",
       "         '(': 1,\n",
       "         'execute': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change this\n",
    "freqdict = get_freq_dict(processed_unk, 2)\n",
    "freqdict[('public',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'platform': 1,\n",
       "         '</s>': 788,\n",
       "         'else': 126,\n",
       "         'first': 16,\n",
       "         'sb': 8,\n",
       "         'catch': 62,\n",
       "         'finally': 13,\n",
       "         '}': 235,\n",
       "         's': 1,\n",
       "         'case': 1,\n",
       "         'return': 64,\n",
       "         'byte': 2,\n",
       "         'try': 7,\n",
       "         'current': 3,\n",
       "         'this': 5,\n",
       "         'boolean': 13,\n",
       "         'buf': 1,\n",
       "         'final': 4,\n",
       "         ';': 7,\n",
       "         '@': 4,\n",
       "         'int': 8,\n",
       "         'if': 52,\n",
       "         'from': 2,\n",
       "         'secret': 1,\n",
       "         'block': 1,\n",
       "         '\"': 12,\n",
       "         'tstatus': 1,\n",
       "         'quoted': 1,\n",
       "         'break': 43,\n",
       "         'struct': 4,\n",
       "         'switch': 23,\n",
       "         'iprot': 31,\n",
       "         'last': 4,\n",
       "         'row': 4,\n",
       "         'super': 3,\n",
       "         ')': 29,\n",
       "         'check': 1,\n",
       "         'for': 5,\n",
       "         'shuffle': 1,\n",
       "         'map': 2,\n",
       "         'verify': 2,\n",
       "         'sorter': 4,\n",
       "         ',': 16,\n",
       "         'connection': 1,\n",
       "         'throw': 15,\n",
       "         'spark': 2,\n",
       "         'oprot': 15,\n",
       "         'logger': 1,\n",
       "         'list': 3,\n",
       "         'help': 1,\n",
       "         'log': 1,\n",
       "         'is': 1,\n",
       "         'assert': 4,\n",
       "         'record': 1,\n",
       "         'bloom': 1,\n",
       "         'ttype': 1,\n",
       "         'org': 4,\n",
       "         'string': 4,\n",
       "         'auth': 1,\n",
       "         'write': 1,\n",
       "         'file': 1,\n",
       "         'agg': 1,\n",
       "         'stream': 1,\n",
       "         '<UNK>': 1,\n",
       "         'do': 1,\n",
       "         'set': 1,\n",
       "         'prefix': 1,\n",
       "         'db': 1,\n",
       "         'total': 2,\n",
       "         'ctx': 1,\n",
       "         'runs': 2,\n",
       "         'hive': 1,\n",
       "         'with': 1,\n",
       "         'buffers': 1,\n",
       "         'callback': 1,\n",
       "         'data': 1,\n",
       "         'ensure': 1,\n",
       "         'state': 1,\n",
       "         'len': 1,\n",
       "         'system': 6,\n",
       "         'jsc': 1,\n",
       "         'long': 2,\n",
       "         'private': 1,\n",
       "         'double': 1,\n",
       "         'array': 1,\n",
       "         'class': 1,\n",
       "         'start': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdict[('}',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b5) The langauge model\n",
    "\n",
    "We are now ready to put everything together and make our langauge model! Below we have written the function `build_ngram_model(codefile, n)` that takes in a text file, the value `n` that signals the order of our n-gram, and returns a language model in the form of a `namedtuple`. All we need to do is to call the various functions you've implemented so far in (c)! (**No implementation required here**).\n",
    "\n",
    "A `namedtuple` is a versatile data structure that allows one to associate multiple data fields with one variable. Below, we created a `namedtuple` called `LanguageModel`; this `LanguageModel` consists of the following information: the n-gram order `n`, the frequency distribution dictionary `fd`, and the vocabulary (as a `set` of words) `vocab`. Now, after we make a `LanguageModel`, we will be able to access these fields using the `dot` operator, for example, `toy_lm.vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY THIS CELL\n",
    "from collections import namedtuple\n",
    "\n",
    "def build_ngram_model(codefile, n):\n",
    "    LanguageModel = namedtuple('LanguageModel', ['n', 'fd', 'vocab'])\n",
    "    psents = process_code(codefile)\n",
    "    vocab = get_vocab(psents)\n",
    "    psentsunk = process_unk(psents, vocab)\n",
    "    fd = get_freq_dict(psentsunk, n)\n",
    "    return LanguageModel(n, fd, vocab)\n",
    "\n",
    "toy_lm = build_ngram_model(\"../datasets/hw2/apache_spark.short.txt\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b6) Log conditional probabilities. (10 points)\n",
    "\n",
    "The heart of the language model above is just the frequency dictionary. To make our model functional, we will need to use the frequency dictionary to calculate (log) conditional probabilities. Write a function `log_prob(lm, context, token)` that takes in a language model `lm`, `context` in the form of a tuple, and a `token`. The function returns the *add-1 (Laplacian) smoothed* conditional probability values `log P(token|context)`.\n",
    "\n",
    "We would like to calculate log probabilities, instead of raw probability values, because ultipmately we will use the language model to calculate the likelihood of a text. Multiplying many very small raw probability values may result in underflow issues (and inaccuracies) in any programming language.\n",
    "\n",
    "You will need to get the size of the vocabulary when writing this function. **Keep in mind** that the `<UNK>`, `<s>`, and `</s>` symbols should all be a part of your vocabulary.\n",
    "\n",
    "```\n",
    ">>> log_prob(toy_lm, ('}',), '</s>')\n",
    "-1.965301477025893\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_prob(lm, context, word) -> float:\n",
    "    sample_size = lm.fd[context][word] + 1\n",
    "    universe_size = sum(lm.fd[context].values()) + len(lm.vocab) + 3\n",
    "    return math.log2(sample_size / universe_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9653014770258928"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change\n",
    "log_prob(toy_lm, ('}',), '</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b7) Perplexity (16 points)\n",
    "\n",
    "Our final step to make our langauge model functional would be to calculate perplexity of a document (e.g., a new text file). Implement the function `get_ppl(lm, newfile)` that returns the perplexity of `newfile` given language model `lm` that you built using `build_ngram_model`.\n",
    "\n",
    "Be mindful to check whether a word appears in the language model's vocabulary; if not, replace with `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> get_ppl(toy_lm, \"data/test.java.txt\")\n",
    "77.40867815861749\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_ppl(lm, testfile) -> float:\n",
    "    lines = process_unk(process_code(testfile), lm.vocab)\n",
    "    context_size = lm.n - 1\n",
    "    P = N = 0\n",
    "    for line in lines:\n",
    "        N += 1 # I think the TA is counting <s> at the beggining as a token\n",
    "        for i, token in enumerate(line + ['</s>']):\n",
    "            padding = ['<s>'] * max(0, context_size - i)\n",
    "            context = padding + line[max(0, i - context_size):i]\n",
    "            P -= log_prob(lm, tuple(context), token)\n",
    "            N += 1\n",
    "    return 2**(P/N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.40867815856888"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change\n",
    "get_ppl(toy_lm, \"../datasets/hw2/test.java.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Project attribution (10 points)\n",
    "\n",
    "If we have built language models of diffeernt projects, they can be used to check which project the unknown piece of code come from. Concretely, for the unknown lines of code, we can run the file through each known project's language model, and use perplexity to predict which project the unknown code is most likely to belong to.\n",
    "\n",
    "In this problem, build two **bigram** models:\n",
    "- a spark language model\n",
    "- a gson language model\n",
    "\n",
    "Make sure to train these models from the full text. Once you have trained both models, use the `get_ppl` function from each language model on the file `test.java.txt` to get the perplexity. \n",
    "\n",
    "Which project does the code come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_model = build_ngram_model(\"../datasets/hw2/apache_spark.large.txt\", 2)\n",
    "gson_model = build_ngram_model(\"../datasets/hw2/google_gson.large.txt\", 2)\n",
    "\n",
    "# Your code goes here: \n",
    "\n",
    "spark_ppl = get_ppl(spark_model, \"../datasets/hw2/test.java.txt\")\n",
    "gson_ppl = get_ppl(gson_model, \"../datasets/hw2/test.java.txt\")\n",
    "more_likely_project = \"spark\" if spark_ppl < gson_ppl else \"gson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for spark: 49.39873845439152\n",
      "Test perplexity for gson: 125.53583007269333\n",
      "The more likely Project is spark\n"
     ]
    }
   ],
   "source": [
    "# Do not change this\n",
    "\n",
    "print(\"Test perplexity for spark: \" + str(spark_ppl))\n",
    "print(\"Test perplexity for gson: \" + str(gson_ppl))\n",
    "print(\"The more likely Project is \" + more_likely_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## (d) Random code generator (5 points)\n",
    "\n",
    "Now, we are ready to generate some Java functions! Starting with the start symbol `<s>`, at each step, use the previously generated token as context, and sample one of the top 5 tokens that follow this token. We stop whenever we hit `</s>`, or when we have generated a 100-token function, whichever is earlier.\n",
    "\n",
    "Implement a function `code_generator(bigramlm, k)` that takes a bigram langauge model---trained on our apache_spark.large.txt---as input, and generates `k=5` random sentences.\n",
    "\n",
    "**NOTE**: We do not want to generate `<UNK>`, so always remove `<UNK>` when it exists in the next tokens that follow the given context.\n",
    "If you use the random seed 7 and randint to pick one token from the top 5 tokens, you will get the following result.\n",
    "\n",
    "```\n",
    ">>> funcs = code_generator(spark_model, 2)\n",
    ">>> funcs[0]\n",
    "'private static long ( ) , \" + i = ( \" ) ; return last comparison ; } if not found \" ) { case 1 ; struct begin ( \" ) , new illegal index ) , \" a string get class ( new hash map ( ) , \" a \" a , 0 ] = = 0 ] = = = null ; if ( new illegal uao _ args struct field begin ( ) , 0 , string > 0 . get ( new hash set status ) , 0 ] = new tuple2 : '\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def _get_most_common_tokens(lm, ctx, mc):\n",
    "    return [token for token, count in lm.fd[tuple(ctx)].most_common(mc)]\n",
    "\n",
    "def _generate_random_function(lm, tl=102, mc=5):\n",
    "    context = ['<s>'] * (lm.n - 1)\n",
    "    tokens = ['<s>']\n",
    "    while (len(tokens) < tl) and (tokens[-1] != '</s>'):\n",
    "        next_token = random.choice(_get_most_common_tokens(lm, context, mc))\n",
    "        tokens.append(next_token)\n",
    "        context = context[1:] + [next_token]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def code_generator(bigramlm, n) -> List[str]:\n",
    "    return [_generate_random_function(bigramlm)[1:-1] for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'private static long ( ) , \" + i = ( \" ) ; return last comparison ; } if not found \" ) { case 1 ; struct begin ( \" ) , new illegal index ) , \" a string get class ( new hash map ( ) , \" a \" a , 0 ] = = 0 ] = = = null ; if ( new illegal uao _ args struct field begin ( ) , 0 , string > 0 . get ( new hash set status ) , 0 ] = new tuple2 :'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change this \n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "funcs = code_generator(spark_model, 5)\n",
    "' '.join(funcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "64523368cfcc0679ac832abc9db724d4345cf20694ed59ab9f6cbea697d5fbf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
